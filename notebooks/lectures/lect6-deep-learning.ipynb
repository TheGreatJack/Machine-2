{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A brief introduction to deep learning\n",
    "**By: Santiago Hincapie-Potes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Administrative\n",
    "**NO** homework today, yeey! But plz play with keras\n",
    "\n",
    "Review syllabus:\n",
    "* RNN <-> Autoencoders\n",
    "* Overview research area at the beginning of the lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "1. Overview\n",
    "2. Applications\n",
    "3. Training Deep Neural Net\n",
    "    + Neural what?\n",
    "    + Vanishing gradient problem \n",
    "    + Transfer Learning\n",
    "    + Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is deep learning?\n",
    "Deep learning is a class of machine learning algorithms that:\n",
    "* Use a cascade of multiple layers of nonlinear processing units for feature extraction and transformation. Each successive layer uses the output from the previous layer as input.\n",
    "* Learn in supervised (e.g., classification) and/or unsupervised (e.g., pattern analysis) manners.\n",
    "* Learn multiple levels of representations that correspond to different levels of abstraction; the levels form a hierarchy of concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why deep learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](img/d2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](img/d1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](img/d3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Further Reading\n",
    "LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436–444. https://doi.org/10.1038/nature14539\n",
    "\n",
    "\n",
    "Schmidhuber, J. (2015). Deep learning in neural networks: An overview. Neural Networks, 61, 85–117. https://doi.org/10.1016/j.neunet.2014.09.003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Application\n",
    "* Automatic speech recognition\n",
    "* Image recognition\n",
    "* Natural language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Ok and bio?\n",
    "+ [Drug discovery](https://doi.org/10.1016/j.drudis.2018.01.039)\n",
    "+ [Biomarker discovery](http://www.aging.ai/)\n",
    "+ [Proteomics](https://github.com/tavanaei/Cancer-Suppressor-Gene-Deep-Learning)\n",
    "+ [Metabolomics](https://pubs.acs.org/doi/full/10.1021/acs.jproteome.7b00595)\n",
    "+ Genomics\n",
    "    * [Variant calling](https://github.com/google/deepvariant)\n",
    "    * [Gene expression](https://www.biorxiv.org/content/early/2015/12/15/034421)\n",
    "    * [Predicting enhancers and regulatory regions](https://www.nature.com/articles/nmeth.3547)\n",
    "    * [Non-coding RNA](https://link.springer.com/article/10.1007%2Fs13721-016-0129-2)\n",
    "    * [Methylation](https://www.nature.com/articles/srep19598)\n",
    "+ [Systems biology](https://www.nature.com/articles/nmeth.4627/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### More information\n",
    "Ching, T., Himmelstein, D. S., Beaulieu-Jones, B. K., Kalinin, A. A., Do, B. T., Way, G. P., … Greene, C. S. (2017). Opportunities And Obstacles For Deep Learning In Biology And Medicine. Cold Spring Harbor Laboratory. https://doi.org/10.1101/142760\n",
    "\n",
    "\n",
    "Angermueller, C., Pärnamaa, T., Parts, L., & Stegle, O. (2016). Deep learning for computational biology. Molecular Systems Biology, 12(7), 878. https://doi.org/10.15252/msb.20156651\n",
    "\n",
    "\n",
    "Ravi, D., Wong, C., Deligianni, F., Berthelot, M., Andreu-Perez, J., Lo, B., & Yang, G.-Z. (2017). Deep Learning for Health Informatics. IEEE Journal of Biomedical and Health Informatics, 21(1), 4–21. https://doi.org/10.1109/jbhi.2016.2636665"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training Deep Neural Net\n",
    "**By: Santiago Hincapie-Potes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural Nets\n",
    "![](img/d4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "def get_model():\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='tanh', input_dim=784))\n",
    "    model.add(Dense(200, activation='tanh'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.reset_states()\n",
    "    return model\n",
    "\n",
    "model = get_model_A()\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='SDG',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(data.x_test, data.y_test, epochs=40, batch_size=64, validation_data=(data.x_test, data.y_test))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Train neural nets\n",
    "* Gradient Descent\n",
    "* Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](img/d7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Gradient Descent\n",
    "![](img/d5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropagation\n",
    "![](img/d6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Faster Optimizers\n",
    "* Momentum optimization\n",
    "* Nesterov Accelerated Gradient\n",
    "* AdaGrad\n",
    "* RMSProp\n",
    "* Adam Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](http://ruder.io/content/images/2016/09/contours_evaluation_optimizers.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Mini-batch SGD\n",
    "```python\n",
    "while True:\n",
    "    data_batch = dataset.sample_data_batch()\n",
    "    loss = network.forward(data_batch)\n",
    "    dx = network.backward()\n",
    "    x += - learning_rate * dx\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](img/d12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Momentum optimization\n",
    "```python\n",
    "while True:\n",
    "    data_batch = dataset.sample_data_batch()\n",
    "    loss = network.forward(data_batch)\n",
    "    dx = network.backward()\n",
    "    v = mu * v - learning_rate * dx\n",
    "    x += v\n",
    "```\n",
    "* Physical interpretation as ball rolling down the loss function + friction (mu coefficient).\n",
    "* mu = usually ~0.5, 0.9, or 0.99 (Sometimes annealed over time, e.g. from 0.5 $\\to$ 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Nesterov Accelerated Gradient\n",
    "![](img/d13.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "while True:\n",
    "    data_batch = dataset.sample_data_batch()\n",
    "    loss = network.forward(data_batch)\n",
    "    dx = network.backward()\n",
    "    v_prev = v\n",
    "    v = mu * v - learning_rate * dx\n",
    "    x += -mu * v_prev + (1 + mu)*v\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### AdaGrad\n",
    "```python\n",
    "while True:\n",
    "    data_batch = dataset.sample_data_batch()\n",
    "    loss = network.forward(data_batch)\n",
    "    dx = network.backward()\n",
    "    cache += dx**2 # Second moment\n",
    "    x += -learning_rate * dx / (np.sqrt(cache) + 1e-7)\n",
    "```\n",
    "* every single dimension of your parameter space, now has its own kind of learning rate that scaled dynamically based on what kind of gradients you're seening in terms of their scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/d14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp\n",
    "```python\n",
    "while True:\n",
    "    data_batch = dataset.sample_data_batch()\n",
    "    loss = network.forward(data_batch)\n",
    "    dx = network.backward()\n",
    "    cache += dx**2\n",
    "    x += -learning_rate * dx / (np.sqrt(cache) + 1e-7)\n",
    "```\n",
    "Introduced in a slide in Geoff Hinton’s Coursera class, lecture 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam = Adaptive Moment Estimation $\\approx$  RMSProp + momentum\n",
    "\n",
    "```python\n",
    "while True:\n",
    "    data_batch = dataset.sample_data_batch()\n",
    "    loss = network.forward(data_batch)\n",
    "    dx = network.backward()\n",
    "    # Incomplete need a bias correction\n",
    "    m = beta1 * m + (1 - beta1) * dx\n",
    "    v = beta2 * v + (1 - beta2) * (dx**2)\n",
    "    x += -learning_rate * m/(np.sqrt(v) + 1e-7)\n",
    "```\n",
    "Adam is a good default choice in most cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Stochastic gradient descent**\n",
    "```python\n",
    "optimizers.SGD(lr=0.01)\n",
    "```\n",
    "**Momentum optimization**\n",
    "```python\n",
    "optimizers.SGD(lr=0.01, momentum=0.9)\n",
    "```\n",
    "**Nesterov Accelerated Gradient**\n",
    "```python\n",
    "optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "```\n",
    "**AdaGrad**\n",
    "```python\n",
    "optimizers.Adagrad(lr=0.01)\n",
    "```\n",
    "**RMSProp**\n",
    "```python\n",
    "optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "```\n",
    "**Adam Optimization**\n",
    "```python\n",
    "optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**How to use**\n",
    "```python\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Vanishing Gradients Problems\n",
    "Gradients often get smaller and smaller as the algorithm progresses down to the lower layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "it is only around 2010 that significant progress was made in understanding it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](img/d8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Xavier and He Initialization\n",
    "$$ Var(w_i) = \\frac{2}{N_{in} + N_{out}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "| Activation function| Uniform $[-r, r]$                          | Normal distribution                      |\n",
    "|--------------------|--------------------------------------------|------------------------------------------|\n",
    "| Logistic           | r = $\\sqrt{\\frac{6}{n_{in} + n_{out}}}$    | $\\sqrt{\\frac{2}{n_{in} + n_{out}}}$      |\n",
    "| $\\tanh$            | r = $\\sqrt[4]{\\frac{6}{n_{in} + n_{out}}}$ |r = $\\sqrt[4]{\\frac{2}{n_{in} + n_{out}}}$|\n",
    "| ReLU and variants  | r = $\\sqrt[\\sqrt2]{\\frac{6}{n_{in} + n_{out}}}$|r = $\\sqrt[\\sqrt2]{\\frac{2}{n_{in} + n_{out}}}$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "model.add(Dense(64,\n",
    "                activation='sigmoid',\n",
    "                kernel_initializer='he_normal')) # he_uniform, xaviar by default\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Initialization is an active research field**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Nonsaturating Activation Functions\n",
    "One of the insights in the 2010 paper by Glorot and Bengio was that the vanishing/\n",
    "exploding gradients problems were in part due to a poor choice of activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Until then most people had assumed that if Mother Nature had chosen to use roughly sigmoid activation functions in biological neurons, they must be an excellent choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* ReLU rocks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* dying ReLUs :c<br>\n",
    "during training, some neurons effectively die, meaning\n",
    "they stop outputting anything other than 0. In some cases, you may find that half of\n",
    "your network’s neurons are dead, especially if you used a large learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Use: leaky ReLU, SeLU or eLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Leaky ReLU**\n",
    "```python\n",
    "keras.activations.relu(x, alpha=0.1, max_value=None, threshold=0.0)\n",
    "```\n",
    "\n",
    "**SeLU**\n",
    "```python\n",
    "keras.activations.selu(x, alpha=0.1, max_value=None, threshold=0.0)\n",
    "```\n",
    "\n",
    "**eLU**\n",
    "```python\n",
    "keras.activations.elu(x, alpha=1.0)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Use normalized data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Shuffle data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Dropout\n",
    "\"randomly set some neurons to zero in the forward pass\"\n",
    "![](img/d10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How could this possibly be a good idea?\n",
    "* Forces the network to have a redundant representation.\n",
    "* Dropout is training a large ensemble of models (that share parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "model.add(Dense(200, activation='tanh'))\n",
    "model.add(Dropout(rate=0.5)) # convnet lower value\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Early Stopping\n",
    "![](img/d9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "See `keras.callbacks.EarlyStopping`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Batch normalization\n",
    "To increase the stability of a neural network, batch normalization normalizes the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation.\n",
    "\n",
    "\n",
    "batch normalization allows each layer of a network to learn by itself a little bit more independently of other layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 64)                960       \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 130       \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 5,770\n",
      "Trainable params: 5,510\n",
      "Non-trainable params: 260\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# instantiate model\n",
    "model = Sequential()\n",
    "\n",
    "# we can think of this chunk as the input layer\n",
    "model.add(Dense(64, input_dim=14))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# we can think of this chunk as the hidden layer    \n",
    "model.add(Dense(64, kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# we can think of this chunk as the output layer\n",
    "model.add(Dense(2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# setting up the optimization of our weights \n",
    "sgd = SGD(lr=0.1, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tomorrow:\n",
    "* ConvNets\n",
    "* Trasnfer learning"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
