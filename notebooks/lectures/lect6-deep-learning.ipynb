{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A brief introduction to deep learning\n",
    "**By: Santiago Hincapie-Potes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "1. Overview\n",
    "2. Applications\n",
    "3. Training Deep Neural Net\n",
    "    + Neural what?\n",
    "    + Vanishing gradient problem \n",
    "    + Transfer Learning\n",
    "    + Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is deep learning?\n",
    "Deep learning is a class of machine learning algorithms that:\n",
    "* Use a cascade of multiple layers of nonlinear processing units for feature extraction and transformation. Each successive layer uses the output from the previous layer as input.\n",
    "* Learn in supervised (e.g., classification) and/or unsupervised (e.g., pattern analysis) manners.\n",
    "* Learn multiple levels of representations that correspond to different levels of abstraction; the levels form a hierarchy of concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why deep learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/d2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/d1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/d3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading\n",
    "LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436–444. https://doi.org/10.1038/nature14539\n",
    "\n",
    "\n",
    "Schmidhuber, J. (2015). Deep learning in neural networks: An overview. Neural Networks, 61, 85–117. https://doi.org/10.1016/j.neunet.2014.09.003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application\n",
    "* Automatic speech recognition\n",
    "* Image recognition\n",
    "* Natural language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok and bio?\n",
    "+ [Drug discovery](https://doi.org/10.1016/j.drudis.2018.01.039)\n",
    "+ [Biomarker discovery](http://www.aging.ai/)\n",
    "+ [Proteomics](https://github.com/tavanaei/Cancer-Suppressor-Gene-Deep-Learning)\n",
    "+ [Metabolomics](https://pubs.acs.org/doi/full/10.1021/acs.jproteome.7b00595)\n",
    "+ Genomics\n",
    "    * [Variant calling](https://github.com/google/deepvariant)\n",
    "    * [Gene expression](https://www.biorxiv.org/content/early/2015/12/15/034421)\n",
    "    * [Predicting enhancers and regulatory regions](https://www.nature.com/articles/nmeth.3547)\n",
    "    * [Non-coding RNA](https://link.springer.com/article/10.1007%2Fs13721-016-0129-2)\n",
    "    * [Methylation](https://www.nature.com/articles/srep19598)\n",
    "+ [Systems biology](https://www.nature.com/articles/nmeth.4627/)\n",
    "\n",
    "### More information\n",
    "Ching, T., Himmelstein, D. S., Beaulieu-Jones, B. K., Kalinin, A. A., Do, B. T., Way, G. P., … Greene, C. S. (2017). Opportunities And Obstacles For Deep Learning In Biology And Medicine. Cold Spring Harbor Laboratory. https://doi.org/10.1101/142760\n",
    "\n",
    "\n",
    "Angermueller, C., Pärnamaa, T., Parts, L., & Stegle, O. (2016). Deep learning for computational biology. Molecular Systems Biology, 12(7), 878. https://doi.org/10.15252/msb.20156651\n",
    "\n",
    "\n",
    "Ravi, D., Wong, C., Deligianni, F., Berthelot, M., Andreu-Perez, J., Lo, B., & Yang, G.-Z. (2017). Deep Learning for Health Informatics. IEEE Journal of Biomedical and Health Informatics, 21(1), 4–21. https://doi.org/10.1109/jbhi.2016.2636665"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Neural Net\n",
    "**By: Santiago Hincapie-Potes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Nets\n",
    "![](img/d4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train neural nets\n",
    "* Gradient Descent\n",
    "* Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "![](img/d5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "![](img/d6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/d7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def get_model():\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='tanh', input_dim=784))\n",
    "    model.add(Dense(200, activation='tanh'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.reset_states()\n",
    "    return model\n",
    "\n",
    "model = get_model_A()\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='SDG',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(data.x_test, data.y_test, epochs=40, batch_size=64, validation_data=(data.x_test, data.y_test))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster Optimizers\n",
    "* Momentum optimization\n",
    "* Nesterov Accelerated Gradient\n",
    "* AdaGrad\n",
    "* RMSProp\n",
    "* Adam Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum optimization\n",
    "$$ v_t = \\gamma v_{t-1} + \\nu \\nabla_{\\theta} J(\\theta) $$\n",
    "$$ \\theta = \\theta - v_t $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov Accelerated Gradient\n",
    "$$ v_t = \\gamma v_{t-1} + \\nu \\nabla_{\\theta} J(\\theta - \\gamma v_{t-1}) $$\n",
    "$$ \\theta = \\theta - v_t $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://ruder.io/content/images/2016/09/contours_evaluation_optimizers.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stochastic gradient descent**\n",
    "```python\n",
    "optimizers.SGD(lr=0.01)\n",
    "```\n",
    "**Momentum optimization**\n",
    "```python\n",
    "optimizers.SGD(lr=0.01, momentum=0.9)\n",
    "```\n",
    "**Nesterov Accelerated Gradient**\n",
    "```python\n",
    "optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "```\n",
    "**AdaGrad**\n",
    "```python\n",
    "optimizers.Adagrad(lr=0.01)\n",
    "```\n",
    "**RMSProp**\n",
    "```python\n",
    "optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "```\n",
    "**Adam Optimization**\n",
    "```python\n",
    "optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to use**\n",
    "```python\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing Gradients Problems\n",
    "Gradients often get smaller and smaller as the algorithm progresses down to the lower layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is only around 2010 that significant progress was made in understanding it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/d8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier and He Initialization\n",
    "$$ Var(w_i) = \\frac{2}{N_{in} + N_{out}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Activation function| Uniform $[-r, r]$                          | Normal distribution                      |\n",
    "|--------------------|--------------------------------------------|------------------------------------------|\n",
    "| Logistic           | r = $\\sqrt{\\frac{6}{n_{in} + n_{out}}}$    | $\\sqrt{\\frac{2}{n_{in} + n_{out}}}$      |\n",
    "| $\\tanh$            | r = $\\sqrt[4]{\\frac{6}{n_{in} + n_{out}}}$ |r = $\\sqrt[4]{\\frac{2}{n_{in} + n_{out}}}$|\n",
    "| ReLU and variants  | r = $\\sqrt[\\sqrt2]{\\frac{6}{n_{in} + n_{out}}}$|r = $\\sqrt[\\sqrt2]{\\frac{2}{n_{in} + n_{out}}}$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "model.add(Dense(64,\n",
    "                activation='sigmoid',\n",
    "                kernel_initializer='he_normal')) # he_uniform, xaviar by default\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialization is an active research field**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonsaturating Activation Functions\n",
    "One of the insights in the 2010 paper by Glorot and Bengio was that the vanishing/\n",
    "exploding gradients problems were in part due to a poor choice of activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Until then most people had assumed that if Mother Nature had chosen to use roughly sigmoid activation functions in biological neurons, they must be an excellent choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ReLU rocks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* dying ReLUs :c<br>\n",
    "during training, some neurons effectively die, meaning\n",
    "they stop outputting anything other than 0. In some cases, you may find that half of\n",
    "your network’s neurons are dead, especially if you used a large learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use: leaky ReLU, SeLU or eLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Leaky ReLU**\n",
    "```python\n",
    "keras.activations.relu(x, alpha=0.1, max_value=None, threshold=0.0)\n",
    "```\n",
    "\n",
    "**SeLU**\n",
    "```python\n",
    "keras.activations.selu(x, alpha=0.1, max_value=None, threshold=0.0)\n",
    "```\n",
    "\n",
    "**eLU**\n",
    "```python\n",
    "keras.activations.elu(x, alpha=1.0)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use normalized data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout\n",
    "![](img/d10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "model.add(Dense(200, activation='tanh'))\n",
    "model.add(Dropout(rate=0.5)) # convnet lower value\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early Stopping\n",
    "![](img/d9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `keras.callbacks.EarlyStopping`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch normalization\n",
    "To increase the stability of a neural network, batch normalization normalizes the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation.\n",
    "\n",
    "\n",
    "batch normalization allows each layer of a network to learn by itself a little bit more independently of other layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 64)                960       \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 130       \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 5,770\n",
      "Trainable params: 5,510\n",
      "Non-trainable params: 260\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# instantiate model\n",
    "model = Sequential()\n",
    "\n",
    "# we can think of this chunk as the input layer\n",
    "model.add(Dense(64, input_dim=14))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# we can think of this chunk as the hidden layer    \n",
    "model.add(Dense(64, kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# we can think of this chunk as the output layer\n",
    "model.add(Dense(2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# setting up the optimization of our weights \n",
    "sgd = SGD(lr=0.1, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tomorrow:\n",
    "* ConvNets\n",
    "* Trasnfer learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
